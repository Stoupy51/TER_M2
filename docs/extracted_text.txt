Chapter 6
Quantum Models as Kernel Methods

This chapter is an adapted version of the preprint article “Quantum machine learning
models are kernel methods” by Maria Schuld {1}.

We expose the important link between kernel methods, and quantum circuits used
for supervised learning. We show that a large class of supervised quantum models are
kernel methods with a “quantum kernel” which is fully defined by the data-encoding
Strategy of the circuit. This has far-reaching consequences, for example that such
quantum models can be trained by minimising a relatively simple cost function, and
that their generalisation performance is determined by the data-encoding strategy.

In the previous chapter, we saw that deterministic quantum models can be trained
and used similarly to neural networks by optimising classical control parameters with
gradient descent-type algorithms. However, we also remarked that quantum models
are mathematically quite distinct from neural networks. In this chapter, we will see
that the mathematical framework of quantum computing is instead strikingly similar
to kernel methods that we introduced in Sect. 2.5.4. Both describe how information is
processed by mapping it to vectors that live in potentially inaccessibly large Hilbert
spaces, without the need of ever computing an explicit numerical representation
of these vectors (see Fig.6.1) (2, 3]. This analogy has sparked a range of studies
in the past years, for example, to construct kernelised quantum machine learning
models [4, 5], or to reveal links between quantum machine learning and maximum
mean embeddings [6] as well as metric learning [7].

It turns out that the connection between quantum computing and kernel methods
has far-reaching consequences: most supervised, deterministic quantum models can
fundamentally be formulated as a classical kernel method whose kernel is computed
by a quantum computer. The insight is based on the observation that quantum mod-
els are linear models in a certain feature space [3], and holds both for variational
algorithms presented in the previous chapter, as well as for more sophisticated fault-
tolerant algorithms which we will encounter in Chap.7. It has been a crucial tool
to investigate the separation between the computational complexity of quantum and

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021 217
M. Schuld and F. Petruccione, Machine Learning with Quantum Computers,
Quantum Science and Technology, https://doi.org/10.1007/978-3-030-83098-4_6


218 6 Quantum Models as Kernel Methods

KERNEL METHODS QUANTUM COMPUTING

feature: space

DN

measurements

Fig. 6.1 Quantum computing and kernel methods are based on a similar principle. Both have math-
ematical frameworks in which information is mapped into and then processed in high-dimensional
spaces to which we have only limited access. In kernel methods, the access to the feature space is
facilitated through kernels or inner products of feature vectors. In quantum computing, access to
the Hilbert space of quantum states is given by measurements, which can also be expressed through
inner products of quantum states

classical machine learning [8, 9], and shows that the expressivity, optimisation and
generalisation behaviour of quantum models is largely defined by the data-encoding
strategy or embedding which fixes the kernel. Furthermore, this insight means that
while the kernel itself may explore high-dimensional state spaces of the quantum sys-
tem, quantum models can be trained and operated in a low-dimensional subspace. In
contrast to the strategy of training variational models, we do not have to worry about
finding the right ansatz, or about how to avoid barren plateaus (see Sect. 5.3.3)—
but pay the price of having to compute pairwise distances between embedded data
points.

Since kernel theory is not very easy to understand, the next section will give an
overview of the link between deterministic quantum models and kernel methods
before jumping into more details. Wonderful textbooks on kernel methods are [10,
11], which serve as a basis for many of the following insights.

6.1 The Connection Between Quantum Models and Kernel
Methods

First, a quick overview of the scope. We will consider deterministic quantum models
as specified in Definition 5.1, and where the circuit consists of a data embedding and
a variational part as in Eq. (5.3). However, it will be useful to consider the variational
part and the measurement together as a variational measurement. Mathematically,
this simply means we identify MM, = W(6)’. MW(@) as the observable. Training a
quantum model then means finding the measurement which minimises acost function
that depends on the training data. The important part of these assumptions is that the
embedding is fixed and not trainable as, for example, proposed in [7, 12].

The bridge between quantum machine learning and kernel methods is formed by
the observation that quantum models map data into a high-dimensional feature space,
in which the measurement defines a linear decision boundary as shown in Fig. 6.2.
Note that for this to hold, we need to define the data-encoding density matrices


6.1 The Connection Between Quantum Models and Kernel Methods 219

feature space
"a
( data space ¥

ae

measurement M

of quantum model defines

linear decision boundary
in feature space

Fig. 6.2 Quantum models as linear models in a feature space. A quantum model can be understood
as a model that maps data into a feature space in which the measurement defines a linear decision
boundary. This feature space is not identical to the Hilbert space of the quantum system. Instead,
we can define it as the space of complex matrices enriched with the Hilbert-Schmidt inner product

p(x) = |o(x))(@(x)| as the feature “vectors”! instead of the Dirac vectors |6(.x))
(which was the 2 version of the feature map introduced in Sect. 6.4.1). We can,
therefore, consider the space of complex matrices enriched with the Hilbert-Schmidt
inner product as the feature space of a quantum model and state:

1. Quantum models are linear models in the “feature vectors" p(x). The (potentially train-
able) measurement observable corresponds to the “weight vector”.

As famously known from support vector machines [10], linear models in feature
spaces can be efficiently evaluated and trained if we have access to inner products
of feature vectors, which we saw in Sect. 2.5.4 is a function « in two data points
x, x’ called the kernel. Kernel theory essentially uses linear algebra and functional
analysis to derive statements about the expressivity, trainability and generalisation
power of linear models in feature spaces directly from the kernel. For us, this means
that we can learn a lot about the properties of quantum models if we study inner
products «(x, x’) = tr{p(x’)(x)} (or, for pure states, K(x, x’) = [o(x')1o@))P
which we will call quantum kernels.

To understand what kernels can tell us about quantum machine learning,
need another important concept from kernel theory: the reproducing kernel Hilbe
space (RKHS). An RKHS is an alternative feature space of a kernel, and therefore,
reproduces all observable behaviour of the machine learning model. More precisely,
it is a feature space of functions x > g,(-) = K(x, +), which are constructed from
the kernel. The RKHS contains one such function for every input x € ¥ from the
input domain, as well as their linear combinations (for example, for the popular
Gaussian kernel these linear combinations are sums of Gaussians centred in the
individual data points). In an interesting—and by no means trivial—twist, these
functions happen to be identical to the linear models in feature space. For quantum
machine learning, this means that the space of quantum models and the RKHS of

' The term feature vectors derives from the fact that they are elements of a vector space, not that
they are vectors in the sense of the space C% or RY.


220 6 Quantum Models as Kernel Methods

feature space F

data-encoding ed
> i: feature map RR / space of quantum ‘»
¢ data space’ V } ajsn ina Mian models
re n(z,2') = tr{a(z)olz')} -) = tr{p(-
Tie a $0) = te{0)M}
yo ~~ 10)
canonical / J=K . zt i
saa [ LO = Hl) yy Se

reproducing kernel
Hilbert space F

Fig. 6.3. Overview of the link between quantum models and kernel methods. The strategy with
which data is encoded into quantum states is a feature map from the space of data to the feature
space ¥ of density matrices p. In this space, quantum models can be expressed as linear models
whose decision boundary is defined by the measurement. According to kernel theory, an alternative
feature space with the same kernel is the RKHS F, whose vectors are functions arising from fixing
one entry of the kernel (i.e., the inner product of data-encoding density matrices). The RKHS is
equivalent to the space of quantum models, which are linear models in the data-encoding feature
space. These connections can be used to study the properties of quantum models as learners, which
turn out to be largely determined by the kernel, and therefore, by the data-encoding strategy

the quantum kernel contain exactly the same functions (see Sect.6.4.2). What we
gain is an alternative representation of deterministic quantum models, one that only
depends on the quantity tr{p(x’)p(x)} (see Fig. 6.3).

This alternative representation can be very useful for all sorts of things. For exam-
ple, it allows us to study the universality of quantum models as function approxima-
tors by investigating the universality of the RKHS, which, in turn, is a property of
the quantum kernel. But probably the most important use is to study optimisation:
minimising typical cost functions over the space of quantum models is equivalent
to minimising the same cost over the RKHS of the quantum kernel (see Sect. 6.5.1).
The famous representer theorem uses this to show that “optimal models” (i.e., those
that minimise the cost) can be written in terms of the quantum kernel as

M M
fopt(x) = J? amtr(o(x")p(x)} = tr | Yonpts") wo} » ©.1)
=1

m=1

where x”, m = 1,..., M are the training data samples and a, € R (see Sect. 6.5.2).
Looking at the expression in the round brackets, this enables us to say something
about optimal measurements for quantum models:

2. Quantum models that minimise typical machine learning cost functions have measure-
ments that can be written as “kernel expansions in the data”, M = Ym, Omp(x").


6.1 The Connection Between Quantum Models and Kernel Methods 221

In other words, we are guaranteed that the best measurements for machine learning
tasks only have M < 2" degrees of freedom {a,,}, where n is the number of qubits.
Even more, if we include a regularisation term into the cost function, the kernel
defines entirely which models are actually penalised or preferred by regularisation.
Since the kernel only depends on the way in which data is encoded into quantum
states, one can conclude that data encoding fully defines the minima of a given cost
function used to train quantum models (see Sect. 6.5.3).

But how can we find the optimal model in Eq. (6.1)? We could simply train a
variational ansatz, hoping that it learns the right measurement basis. But as illustrated
in Fig. 6.4, variational training typically only searches through a small subspace of
all possible quantum models/measurements. This has a good reason: to train a circuit
that can express any quantum model (and is hence guaranteed to find the optimal
one) would require parameters for all O(2?") degrees of freedom, which is intractable
for all but toy models. However, also here kernel theory can help: not only is the
optimal measurement defined by M < 2" degrees of freedom, finding the optimal
measurement has the same favourable scaling (see Sect. 6.5.4) if we switch to a
kernel-based training approach.

3. The problem of finding the optimal measurement for typical machine learning cost func-
tions trained with M data samples can be formulated as an M-dimensional optimisation
problem.

If the loss is convex, as is common in machine learning, the optimisation prob-
lem is guaranteed to be convex as well. Hence, under rather general assump-
tions, we are guaranteed that the hard problem of picking the best quantum model
shown in Eq. (6.1) is tractable and of a simple structure, even without reverting to
variational heuristics. In addition, convexity—the property that there is only one
global minimum—may help with trainability problems like the notorious “barren
plateaus” [13] in variational circuit training. If the loss function is the hinge loss,
things reduce to a standard support vector machine with a quantum kernel, which is
one of the algorithms proposed in [2, 3].

The remainder of the chapter will essentially follow the structure of this synopsis
to discuss every statement in more mathematical detail.

6.2 Quantum Computing, Feature Maps and Kernels

Let us start by laying the groundwork for the kernel perspective on quantum machine
learning. First, we review the link between the process of encoding data into quantum
states and feature maps and construct the “quantum kernel” that we will use through-
out. We will then give some examples of data-encoding feature maps encountered
in Chap. 4 and their quantum kernels, including a general description derived from
the Fourier formalism in Sect. 5.2, which allows us to understand these kernels via
trigonometric sums.


222 6 Quantum Models as Kernel Methods

space of quantum models

Cost(a)
Cost()

@

kernel-based training variational training

Fig. 6.4 Kernel-based training versus variational training. Training a quantum model as defined in
this chapter tries to find the optimal measurement Mop over all possible quantum measurements.
Kernel theory guarantees that in most cases this optimal measurement will have a representation
that is a linear combination in the training data with coefficients @ = (aj,.... ays)”. Kernel-
based training, therefore, optimises over the parameters a directly, effectively searching for the best
model in an M-dimensional subspace spanned by the training data (blue). We are guaranteed that

of = Mop, and if the loss is convex, this is the only minimum, which means that kernel-based
training will find the best measurement out of all measurements. Variational training parametrises
the measurement instead by a general ansatz that depends on K parameters 0 = (0),..., Ox), and
tries to find the optimal measurement M3 in the subspace explored by the ansatz. This 0-subspace
is not guaranteed to contain the globally optimal measurement Mop, and optimisation is usually
non-convex. We are, therefore, guaranteed that kernel-based training finds better or the same minima
to variational training, but at the expense of having to compute pairwise distances of data points for
training and classification

6.2.1 Data Encoding as a Feature Map

Consider a data embedding circuit S(x) that depends on data x € Y from some
domain # to prepare a quantum state S(x)|0) = |¢(x)). While from a quantum
physics perspective it seems natural—and has been done in Sect. 4.5—1to think of
x — |G(x)) as the feature map that links quantum computing to kernel methods, we
will see below that quantum models are not linear in the Hilbert space of the quantum
system, which means that the apparatus of kernel theory does not apply elegantly.
Instead, we will solely work with x > p(x) = |@(x))((x)| as the “quantum feature
map” and, to avoid confusion, call it the data-encoding feature map.

Definition 6.1 (Data-encoding feature map) Given a n-qubit quantum system with
states |), and let F be the space of complex-valued 2" x 2"-dimensional matrices
equipped with the Hilbert-Schmidt inner product (p, o)- = tr{p*a} for p,o € F.
The data-encoding feature map is defined as the transformation

OX > F, G(X) =16(2) (G(x) = pC), (6.2)

and can be implemented by a data-encoding quantum circuit S(x).

This definition makes sure that the feature space is a Hilbert space, and it allows
measurements to live in the same space [14], which we will need to define linear


6.2 Quantum Computing, Feature Maps and Kernels 223

models in F. Section 6.2.3 will discuss that this definition of the feature space is
equivalent to the tensor product space of complex vectors |y) @ |w*) used in [9].
Finally, note that while we limit our scope to pure quantum states here, the data-
encoding feature map can easily be extended to mixed states.

6.2.2 Quantum Kernels

Remember that kernels were defined as real or complex-valued positive definite func-
tions in two data points, K : XY x ¥ — K, where K can be C or R (see Sect. 2.5.4).
For every such function, we are guaranteed that there exist at least one feature map
such that inner products of feature vectors (x) from the feature Hilbert space F
form the kernel, «(x, x’) = (6(x’), 6(x)) x. Vice versa, every feature map gives rise
to a kernel via the inner product in the corresponding feature space. The importance
of kernels for machine learning is that they are a means of “computing” in feature
space without ever accessing or numerically processing the vectors @(x): everything
we need to do in machine learning can be expressed by inner products of feature
vectors, instead of the feature vectors themselves. In the cases that are practically
useful, these inner products can be computed by a comparably simple function. This
makes the computations in intractably large spaces tractable.

With the Hilbert-Schmidt inner product from Definition 6.1 we can immediately
write down the kernel induced by the data-encoding feature map, which we will call
the quantum kernel.

Definition 6.2 (Quantum kernel) Let ¢ be a data-encoding feature map over domain
&. A quantum kernel is the inner product between two data-encoding feature vectors
p(x), p(x’) with x, x’ € X,

K(x, x) = tp(r')a(x)} = (O16(2))?- (6.3)

To justify the term “kernel” we need to show that the quantum kernel is indeed a
positive definite function. The quantum kernel is a product of the complex-valued ker-
nel Ke(x, x’) = (¢(x")|G(x)) and its complex conjugate K(x, x’)* = ((x)|d(e')) =
((x")|b(x))*. Since products of two kernels are known to be kernels themselves,
we only have to show that the complex conjugate of a kernel is also a kernel. For any
x™ € X,m=1...M,and for any cm € C, we have


224 6 Quantum Models as Kernel Methods

Sent (Hetx™, x”) = oe mer (G(x) G(x""))

mm’ mm

= (Zetec) (Dect)
=> chloe) I
>0,

which means that the complex conjugate of a kernel is also positive definite.

6.2.3. Making Sense of Matrix-Valued Feature Vectors

For readers that struggle to think of density matrices as feature vectors, the data-
encoding feature map (and further below, linear models) may be hard to visualise.
We, therefore, want to insert a brief comment on an alternative version of the data-
encoding feature map.

For all matters and purposes, the data-encoding feature map can be replaced by
an alternative formulation

dy: X >F, CHEN, (6.4)
d(x) = Id(x)) @ Id*(x)), (6.5)

where |¢*(x)) denotes the quantum state created from applying the complex conju-
gated (but not transposed) unitary |¢*(x)) = U*(x)|0) instead of |@(x)) = U(x)|0),
and F, is the space of tensor products of a data-encoding Dirac vector with its com-
plex conjugate. Note that since the complex conjugate of a unitary is a unitary, the
unusual notation |¢*(x)) describes a valid quantum state which can be prepared by
a physical circuit. The alternative feature space F, is a subspace of the Hilbert space
H @ H* with the property that inner products are real. One can show (but we won’t
do it here) that F, is indeed a Hilbert space.

The inner product in this alternative feature space F,, is the absolute square of the
inner product in the Hilbert space 1 of quantum states

(bh ple, = Mebly)l?, (6.6)

and is, therefore, equivalent to the inner product in F. This guarantees that it leads
to the same quantum kernel. The subscript v refers to the fact that |d(x)) ® |¢*(x))
is a vectorisation of p(x), which reorders the 2" matrix elements as a vector in C4".

Vectorised density matrices are common in the theory of open quantum sys-
tems [15], where they are written as |p)) (see also the Choi-Jamiolkowski isomor-


6.2 Quantum Computing, Feature Maps and Kernels 225

phism). We will adopt this notation below to replace the Hilbert-Schmidt inner
product tr{p'o} with (p|c)), which can be more illustrative at times. Note that
the vectorised representation of the data-encoding feature map cannot capture mixed
quantum states and is, therefore, less powerful.

6.3. Examples of Quantum Kernels

To fill the definition of the quantum kernel with life, let us revisit information encod-
ing strategies and define the data-encoding feature map, as well as the kernels they
give rise to (sce also [2]; a summary is presented in Table6.1). It has been shown
that there are kernels that cannot be efficiently computed on classical computers [8],
which we will explore in more detail in Sect. 9 on quantum advantages.

6.3.1 Quantum Kernels Derived from Data Encoding

The following strategies to encode data have a resemblance to kernels that can be
found in the classical machine learning literature. This means that, sometimes up to
an absolute square valuc, we can identify them with standard kernels such as the poly-
nomial or Gaussian kernel. These kernels are plotted in Fig.6.5 using simulations
of quantum computations implemented in the quantum machine learning software
library PennyLane [16]. Note that, as usual, we switch to bold notation when the
input space is CY or RY.

Basis encoding. The data-encoding feature map of basis encoding maps a binary
string to a computational basis state,

G:x > fix) ixl- (6.7)

Table 6.1 Overview of data-encoding strategies used in the literature and their quantum kernels.
If bold notation is used, the input domain is assumed to be the 1 € RY

Encoding Kernel x(x, x’)

Basis encoding Ox x

Amplitude en coding 7 : Ixtx’/?

Repeated amplitude encoding (x*x'[?)"

“Angle encoding Oe Te bcostxy — xx)?

= bs
Coherent state encoding en hx

General time-evolution encoding Dene re" s.t



226 6 Quantum Models as Kernel Methods
The quantum kernel is given by the Kronecker delta
(x, x’) = [iv Lic)? = See's (6.8)

which is of course a very strict similarity measure on input space, and arguably not
the best choice of data encoding for quantum machine learning tasks.

Amplitude encoding. The data-encoding feature map of amplitude encoding asso-

ciates each input with a quantum state whose amplitudes in the computational basis
are the elements in the input vector

N
O:x > [x(x] = D> xixFli) Cl. (6.9)
ij=l
The quantum kernel is the absolute square of the linear kernel
w(x, x’) = [(x"]x) |? = [x"x’?. (6.10)
It is obvious that this quantum kernel does not add much power to a linear model in
the original feature space, and it is more of interest for theoretical investigations that
want to eliminate the effect of the feature map.
Repeated amplitude encoding. Amplitude encoding can be repeated r times,
@:xX —> [x)(x| @--- ® |x) (x] (6.11)
to get powers of the quantum kernel above,
w(x, x') = ((x'Ix) PP) = (1) xP (6.12)

A constant non-homogenity can be added by extending the original input with con-
stant dummy features.

Rotation encoding. The data-encoding feature map of this time-evolution encoding
executed by Pauli rotations is given by

2x > 16(*))(G(X)] (6.13)

with, if we use Pauli-Y rotations,

1 n
16)) = YS T]costae)® sin(xs)!“ Ign, + Gn), (6.14)

Ure Gn=0 k=l


6.3. Examples of Quantum Kernels 227

and the corresponding quantum kernel is related to the cosine kernel:

n n
K(x, Xx) = Tl | sin xx sin x; + COs x, cos x; |” = T] | cos(x, — xpi. (6.15)
k=1 k=1

Coherent state encoding. Finally, we want to add a kernel from a slightly different
computational model, which implements the ubiquitous Gaussian kernel. Coherent
states are known in the field of quantum optics as a description of light modes.
Formally, they are superpositions of so-called Fock states, which are basis states
from an infinite-dimensional discrete basis {|0),|1), |2), ...}, instead of the binary
basis of qubits. A coherent state has the form

ai?

la) = cFy (6.16)

fora € C. Encoding areal scalar input x; € R into acoherent state |@,,), corresponds
to a data-encoding feature map with an infinite-dimensional feature space

k

$x; > lox, )(or,|, with Jay,) = em yw, (6.17)
k=0
We can encode a real vector x = (41, ..., X,)7 into n joint coherent states
Jory) (ex | = leer, ) (Ox, | @ +++ @ lex, ) (Ox, |- (6.18
The quantum kernel is a Gaussian kernel [17]:
K(x, x’) = oe (F +f) ‘ ene? (6.19)

Preparing coherent states can be done with displacement operations in quantum
photonics.

6.3.2. Fourier Representation of Quantum Kernels

The reason that all embeddings plotted in Fig. 6.5 have a periodic, trigonometric
structure lies in an effect we have seen before in Sect. 5.2, where we showed that a
quantum model can be written as a Fourier-type sum, which is a linear combination of
trigonometric functions. Here we will use the same fundamental calculation to define
a general class of embeddings that is used a lot in near-term quantum machine learn-



228° 6 Quantum Models as Kernel Methods

basis embedding amplitude embedding repeated amplitude embedding angle embedding
"
1 1 1 1
° \ os
. t, /i 00
eo ° VY V o £ Bose
1 1 1 ‘
-1 0 -1 ° 3
. . 0 eG 0 as

Fig. 6.5 Quantum kernels of different data embeddings. Plots of some of the functions «(X, x)
for the kernels introduced above, using x = (x), x2) € R? for illustration purposes. The first entry
X is fixed at X = (0, 0) for basis and angle embedding, and at X = (ye dD for the variations of

amplitude embedding. The second value is depicted as the x-y plane

ing, and which includes all examples above if we allow for classical pre-processing
of the features. This strategy assumes that 4 = R" for some arbitrary N (whose
relation to the number of qubits n depends on the embedding), which means that
we will stick to the bold notation. As before, the embedding of x; is executed by
time-evolution encoding, which uses gates of the form e~!"'% where G; is d; < 2"-
dimensional Hermitian operator called the generating Hamiltonian. For the popular
choice of Pauli rotations, G; = jo with the Pauli operator a € {a;, ay, @z} (see also
Fig. 6.6). The gates can be applied to different qubits as in angle encoding, or to the
same qubits, and to be general we allow for arbitrary quantum computations between
each encoding gate. For simplicity, we will assume that each input x; is only encoded
once, and that all the encoding Hamiltonians are the same (G, = --- = Gy = G).
The proof works pretty much as sketched in Sect. 5.2.

Theorem 6.1 (Fourier representation of the quantum kernel) Ler & = R™ and S(x)
be a quantum circuit that encodes the data inputs x = (x,,...,%v)" € &X into ann-
qubit quantum state S(x)|0) = |6(x)) via gates of the form e~" fori = 1,...,N.
Without loss of generality G is assumed to be ad < 2"-dimensional diagonal oper-
ator with spectrum A,,..., Ag. Between such data-encoding gates, and before and
after the entire encoding circuit, arbitrary unitary evolutions W, ..., WN*" can
be applied, so that

S(x) = WAND eG WM) @e-inG yO) (6.20)
The quantum kernel K(x, x’) can be written as

K(x, x’) = ys lett on. (6.21)

s,teQ

where 2 © R™, andcs € C. Foreverys, t € Qwe have —s, —t € Qandcy = Cris
which guarantees that the quantum kernel is real-valued.

For the important class of Pauli generators, the kernel becomes a Fourier series.


6.3 Examples of Quantum Kernels 229

05 05 05 ~
0% Fz

-0.5 -05 -o5 fp oo
~05 *

3 3 3

-3 0 3 0 -3 0
0 0
3-3 33 3-3

10) 8} 4] 10) -Ps@} eH) (0) x7
eo} = {reu}{etenff non] fRatea)

Fig. 6.6 Kernels generated by rotation embeddings. Plots of the quantum kernel «(X, x) with
X = (0, 0) using a very general data-cncoding strategy that repeats the input encoding into a single-
qubit one, two and three times. It is obvious that the repetition decreases the smoothness of the
kernel by increasing the Fourier basis functions from which the kernel is inherently constructed

Corollary 6.1 (Fourier series representation of the quantum kernel) For the setting
described in Theorem 6.1, if the eigenvalue spectrum of G is such that any difference

Ai — Aj fori, j =1,...,d isin Z, then Q becomes the set of N-dimensional integer-
valued vectors n = (nj,...,nw)", m,...ny € Z. In this case, the quantum kernel
is a multi-dimensional Fourier series
w(x,x')= D> ee cay, (6.22)
nn’eQ

Expressions (6.21) and (6.22) reveal a lot about the structure of quantum kernels,
for example, that they are not necessarily translation invariant, «(x, x’) # g(x — x’),
unless the data-encoding strategy leads to cst = Cst5st = Cs and

K(x, x’) = by el8O—¥) 0. (6.23)

seQ

Since e7!iG gixiG — e-i(xi-%))G | this is true for all data embeddings that encode each
original input into a separate physical subsystem, like angle encoding introduced
above.

Itis an interesting question if this link between data embedding and Fourier basis
functions given to us by physics can help design particularly suitable kernels for
applications, or be used to control smoothness properties of the kernel in a useful
manner.


230 6 Quantum Models as Kernel Methods

6.4 . The RKHS of Quantum Kernels

We will now discuss the observation that quantum models are linear models in the
feature space F of the data-encoding feature map. This automatically allows us to
apply the results of kernel methods to quantum machine learning.

6.4.1 Quantum Models as Linear Models

First, let us define what a linear (machine learning) model in feature space is

Definition 6.3 (Linear model) Let Y be a data domain and ¢: Y — F a feature
map. We call any function
F(x) = (G(x), w) , (6.24)

with w € ¥ a linear model in F.

From this definition, we immediately see that deterministic quantum models are
linear models. In the following, we will refer to these models as f rather than fp to
emphasise that we do not need trainable parameters for the statements in this section
to hold.

Theorem 6.2 (Deterministic quantum models are linear models in data-encoding
feature space) Let f(x) = tr{pM} be a quantum model with feature map @: x €
X — p(x) € F and data domain X. The quantum model f is a linear model in F.

It is interesting to note that the measurement M can always be expressed as a
linear combination )>, 74(x*) of data-encoding states p(x*) where x* € X.

Theorem 6.3 (Quantum measurements are linear combinations of data-encoding
states) Let f4(x) = tr{pM} be a quantum model. There exists a measurement
Mexp € F of the form

Mesp = D> 140(x*) (6.25)
k

with x* € 2, such that f(x) = fae (x) for all x € &.

Proof We can divide M into the part that lies in the image of ¥ and the remainder
R
M = Mesp + R. (6.26)

Since the trace is linear, we have


6.4 The RKHS of Quantum Kernels 231

tr{p(x)M) = tlp(x)Mexp} + tr{o(x) R}- (6.27)

The data-encoding state p(x) only has contributions in ¥, which means that the inner
product tr{p(x) R} is always zero.

Below we will see that optimal measurements with respect to typical machine learn-
ing cost functions can be expanded in the training data only.

Note that the fact that a quantum model can be expressed as a linear model in the
feature space does not mean that it is linear in the Hilbert space of the Dirac vectors
|@(x)), nor is it linear in the data input x. If the measurement depends on trainable
parameters (via a variational circuit applied before the measurement), the model is
also not, in general, linear in the trainable parameters.

As a last comment for readers that prefer the vectorised version of the data-
encoding feature map, by writing the measurement operator M = )>; pili) (ul
in its eigenbasis, we can likewise write a quantum model as the inner product of
a vectorised feature vector |@(x)) @ |¢*(x)) € Fy with some other “measurement
vector” Y>; 1u;1/4i) ®@ Iwi) € Fr.

F(x) = (9) MI6@)) (6.28)
= Do miluilo))? (6.29)
= (((2)1 6" @1) (Do mila) @ 1H), (6.30)

or using the vectorised density matrix notation introduced above
f(x) = (p@) |w), (6.31)

with w = >>; |p).

6.4.2 Describing the RKHS

So far, we were dealing with two different kinds of Hilbert spaces: The Hilbert space
H of the quantum system, and the feature space F that contains the embedded data.
We will now construct yet another feature space for the quantum kernel, but one
derived directly from the kernel and with no further notion of a quantum model.
This time the feature space is a Hilbert space F of functions, and due to its special
construction it is called the reproducing kernel Hilbert space (RKHS). The relevance
of this feature space is that the functions it contains turn out to be exactly the quantum
model functions f (which is a bit surprising at first: this feature space contains linear
models defined in an equivalent feature space!).


232 6 Quantum Models as Kernel Methods

The RKHS F of the quantum kernel can be defined as follows (as per Moore-
Aronsajn’s construction”):

Definition 6.4 (Reproducing kernel Hilbert space) Let X #4 %. The reproducing
kernel Hilbert space of a kernel « over 4 is the Hilbert space F created by completing
the span of functions f : ¥ > R, f(.) = K(x, -), x € & (ie., including the limits of
Cauchy series). For two functions f(-) = 0, aik(x!, +), 9) = Lo, Bx, +) € F,
the inner product is defined as

(fade = > aiBjx(x', x4), (6.32)

y

with a;, 6; € R.

Note that according to Theorem 6.1 the “size” of the space of common quantum
models, and likewise the RKHS of the quantum kernel, are fundamentally limited by
the generators of the data-encoding gates. If we consider & as the quantum kernel,
the definition of the inner product reveals with

(K(x, +), K(X", )) pF = K(X, x’), (6.33)

that x — «(x, -) is a feature map of this kernel (but one mapping data to functions
instead of matrices, which feels a bit odd at first). In this sense, F can be regarded
as an alternative feature space to F. The name of this unique feature space comes
from the reproducing property

(f. R(x, -))e = f(x) forall f € F, (6.34)

which also shows that the kernel is the evaluation functional 5, which assigns f to
f(x). An alternative definition of the RKHS is the space in which the evaluation
functional is bounded, which gives the space a lot of favourable properties from a
mathematical perspective.

To most of us, the definition of an RKHS is terribly opaque when first encountered,
so a few words of explanation may help (see also Fig. 6.7). One can think of the RKHS
as a space whose elementary functions (x, -) assign a distance measure to every data
point. Functions of this form were also plotted in Figs. 6.5 and 6.6. By feeding another
data point x’ into this distance measure, we get the distance between the two points.
As a vector space, F also contains linear.combinations of these building blocks.
The functions living in F are, therefore, linear combinations of data similarities,
just like, for example, kernel density estimation constructs a smooth function by
adding Gaussians centred in the data. The kernel then regulates the “resolution” of
the distance measure, for example, by changing the variance of the Gaussian.

Once one gets used to this definition, it is immediately apparent that the functions
living in the RKHS of the quantum kernel are what we defined as quantum models

2 See also www.stals.ox.ac.uk/~sejdinov/teaching/atml 14/Theory_2014.pdf for a great overview.


6.4 The RKHS of Quantum Kernels 233

f= x h(t, t)e F

SS

ak rex

Fig. 6.7 Intuition for the functions living in the reproducing kernel Hilbert space (RKHS). The
RKHS F contains functions that are linear combinations of kernel functions where one “slot” is fixed
in a possible data sample x* € 2. This illustration of one such function f € F, using a Gaussian
kernel, shows how the kernel regulates the “smoothness” of the functions in F, as a wider kernel
will simplify f. Since the RKHS is equivalent to the space of linear models that it has been derived
from, the kernel fundamentally defines the class of functions that the linear model can express and
therefore learn

Theorem 6.4 Functions in the RKHS F of the quantum kernel are linear models in
the data-encoding feature space F and vice versa.

Proof The functions in the RKHS of the quantum kernel are of the form f(-) =
Ye We(r', -), with xf € YX. We get

f(x) = oun’, x) (6.35)
P

= Do wtr{a(x*)p(2)} (6.36)
-

= trl nole*)o@)) (6.37)

= t(Mota). (6.38)

Using Theorem 6.3, we know that all quantum models can be expressed by measure-
ments >>, ~yzp(x*), and hence by functions in the RKHS.

In fact, the above observation applies to any linear model in a feature space that gives
rise to the quantum kernel (see Theorem 4.21 in [10)).

As a first taste of how the connection of quantum models and kernel theory can
be exploited for quantum machine learning, consider the question whether quantum
models are universal function approximators. If quantum models are universal, the
RKHS of the quantum kernel must be universal (or dense in the space of functions we
are interested in). This leads to the definition of a universal kernel (see [10] Definition
4.52)

Definition 6.5 (Universal kernel) A continuous kernel & on a compact metric space
2 is called universal if the RKHS F of k is dense in C(4), ie., for every function g
in the set of functions C(’) mapping from elements in 4 to a scalar value, and for
alle > O there exists an f € F such that


234 6 Quantum Models as Kernel Methods

If —Glloo $€. (6.39)

The reason why this is useful is that there are a handful of known necessary
conditions for a kernel to be universal, for example, if its feature map is injective
(see [10] for more details). This immediately excludes quantum models defined on the
data domain Y = R which use single-qubit Pauli rotation gates of the form e'* (with
o a Pauli matrix) to encode data: since such rotations are 27-periodic, two different
x, x’ € X get mapped to the same data-encoding state p(x). In other words, and to
some extent trivially so, on a data domain that extends beyond the periodicity of a
quantum model we never have a chance for universal function approximation. Other
examples for universal kernels are kernels of the form K(x, x") = 7p) ce (2, x
(see Corollary 4.57 in [10]). Vice versa, the universality proof for a type of quantum
model in [18] suggests that some quantum kernels of the form (6.1) are universal in
the asymptotic limit of exponentially large circuits.

We want to finish with a final note about the relation between “wavefunctions” and
functions in the RKHS of quantum systems (sec also the appendix of [2]). Quantum
states are sometimes called “wavefunctions”, since an alternative definition of the
Hilbert space of a quantum system is the space of functions f(-) = (-) which map
a measurement outcome i corresponding to basis state |i) to an “amplitude” w(i) =
(il). (The dual basis vector (i| can here be understood as the evaluating functional
6; which returns this amplitude.) Hence, the Hilbert space of a quantum system can
be written as a space of functions mapping from {i} > C. But the functions that we
are interested in for machine learning are functions in the data, not in the possible
measurement outcomes. This means that the Hilbert space of the quantum system is
only equivalent to the RKHS of a quantum machine learning model if we associate
data with the measurement outcomes. This is true for many proposals of generative
quantum machine learning models [19, 20].

6.5 Kernel-Based Training

While the question of universality addresses the expressivity of quantum models,
the remaining sections will look at questions of trainability and optimisation, for
which the kernel perspective has the most important results to offer. Notably, we will
see that the optimal measurements of quantum models for typical machine learning
cost functions only have relatively few degrees of freedom. Similarly, the process of
finding these optimal models (i.e., training over the space of all possible quantum
models) can be formulated as a low-dimensional optimisation problem. Most of the
results are based on the fact that for kernel methods, the task of training a model is
equivalent to optimising over the model’s corresponding RKHS.


6.5 Kernel-Based Training 235

6.5.1 Training as Optimising Over the RKHS

As we saw in Sect. 2.3.1, in machine learning we want to find optimal models, or those
that minimise the cost functions derived from learning problems. We also introduced
training as the process of solving an empirical risk minimisation problem. A slightly
more general form called regularised empirical risk minimisation for deterministic
quantum models can be cast as follows:

Definition 6.6 (Regularised empirical risk minimisation of quantum models) Let
X,Y be data input and output domains, p a probability distribution on 4 from
which data is drawn, and L : ¥ x Y x R — [0, 00) a loss function that quantifies
the quality of the prediction of a quantum model f(x) = tr{p(x)M}. Let

Ruf) = I, Ee FON) APC, 9) (6.40)

be the expected loss (or “risk”) of f under L, where L may in general also depend
on x. Since p is unknown, we approximate the risk by the empirical risk

M
Ru = Ley FO). 6.41)

m=)

Regularised empirical risk minimisation of quantum models is the problem of min-
imising the empirical risk over all possible quantum models while also minimising
the norm of the measurement M,

ink AIM + Re(trlo)M)), (6.42)

where \ € R* is a positive hyperparameter that controls the strength of the regular-
isation term.

We saw in Sect. 6.4 that quantum models are equivalent to functions in the RKHS
of the quantum kernel, which allows us to replace the term Rr (tr{p(x)M}) in the
empirical risk by Ri(f), f € F-

But what about the regularisation term? Since with Theorem 6.3, we can write


236 6 Quantum Models as Kernel Methods

IMIlz = tM?) (6.43)
= Do wryitrloc')oe’)} (6.44)
ij
= Dwyane! x/) (6.45)
ij
= (Lows!) ous! De (6.46)
= vy, fr, . (6.47)

the norm of M € F is equivalent to the norm of a corresponding f € F. Hence, the
regularised empirical risk minimisation problem in Eq. (6.42) is equivalent to

inf yf lle + Re(S), (6.48)

which minimises the regularised risk over the RKHS of the quantum kernel. We
will see in the remaining sections that this allows us to characterise the problem of
training and its solutions to a surprising degree.

6.5.2 Optimal Measurements and the Representer Theorem

The representer theorem, one of the main achievements of classical kernel theory,
prescribes that the function f from the RKHS which minimises the regularised
empirical risk can always be expressed as a weighted sum of the kernel between x
and the training data. Together with the connection between quantum models and
the RKHS of the quantum kernel, this fact will allow us to write optimal quantum
machine learning models in terms of the quantum kernel.

More precisely, the representer theorem can be stated as follows (for a more
general version, see [11], Theorem 5.1):

Theorem 6.5 (Representer theorem) Let X,Y be an input and output domain, K :
& x & — Ra kernel with a corresponding reproducing kernel Hilbert space F,
and given training dataD = {(x!, y'),..., (x, y“) © X x Y}. Considera strictly
monotonic increasing regularisation function g: [0,00) — R, and an arbitrary loss
L: Xx YxR- RU {oo}. Any minimiser of the regularised empirical risk

fom = argmin {Ri(f) +9 (IS le)} . (6.49)
Ser

admits a representation of the form


6.5 Kernel-Based Training 237

M
Fopt(*) = D> Om K(X", x), (6.50)

m=)

where a, € Rforalll <m <M.

Note that the crucial difference to the form in Theorem (6.3) is that m does not sum
over arbitrary data from 2’, but over a finite training data set. For us, this means that
the optimal quantum model can be written as

M M
Fopt(®) = > am te{0(x)0@")} = Yo om MOIS)? (6.51)

m=1 m=1

This, in turn, defines the measurements M of optimal quantum models.

Theorem 6.6 (Optimal measurements) For the settings described in Theorem 6.5,
the measurement that minimises the regularised empirical risk can be written as an
expansion in the training data x",m = 1...M

Mop = > Am p(x"), (6.52)

with A» € R.

Proof This follows directly by noting that

M
fopi(X) = > am tr(p(x)p(x")) (6.53)
m=1
M
= tr{o(x) )> amp(x”)} (6.54)
m=1
= tre) Mop} (6.55)

As shown in Fig. 6.4, in variational circuits we typically only optimise over a subspace
of the RKHS since the measurements M are constrained by a particular circuit ansatz.
We can, therefore, not guarantee that the optimal measurement can be expressed by
the variational ansatz. However, the above guarantees that there will always be a
measurement of the form of Eq. (6.52) for which the quantum model has a lower
regularised empirical risk than the best solution of the variational training.

As an example, we can use the apparatus of linear regression to show that the
optimal measurement for a quantum model under least-squares loss can indeed be
written as claimed in Eq. (6.52). For this, we will assume once more that ¥ = RY
where N = 2” andn is the number of qubits, and switch to bold notation. We will also
use the (here much more intuitive) vectorised notation in which the quantum model
f(x) = tr{p(x)M} becomes f(x) = (M |p(x) ), with the vectorised measurement
IM) = Oy rele).


238 6 Quantum Models as Kernel Methods

We saw in Sect. 2.5.1 that the vector w that minimises the least-squares loss of a
linear model f(x) = w’x is given by

w = (X'X)7!x‘y, (6.56)

if the inverse of X*X exist. Here, X is the matrix that contains the data vectors as
rows

Xe en BH
X=]: °..: ], (6.57)
IPE aoe eR

and y is an M-dimensional vector containing the target labels. A little trick exposes
that w can be written as a linear combination of training inputs

w= X' (X(X'X)°X'y) = X'@ = So amx”, (6.58)

m

where a = (a1,..., a)".

Since a quantum model is a linear model in feature space, we can associate the
vectors in linear regression with the vectorised measurement and density matrix, and
immediately derive

-1
IM) = Dy" (D nce" Mes \o(x"))), (6.59)
by making use of the fact that in our notation
X'X <=> Dax”) K(x”) |, (6.60)
and
X'y => Do y"lo(x")). (6.61)

Note that although this looks like an expansion in the feature states, the “coeffi-
cient” of |p(x”)) still contains an operator. However, with Eq. (6.58) and writing
Yn Lox) KX (x)| in its diagonal form

Do loc") Kox™)| = So heel Khel, (6.62)
m k

we have

IM) =} am|a(x")), (6.63)


6.5 Kernel-Based Training 239

with

Om = Ye (he [oCx")) 9" (ia [oy )). (6.64)
k m!
The optimal measurement in “matrix form” reads

M = Pha" p(x") = Dra G(x") 1SX")1, (6.65)

m m

as claimed by the representer theorem. Of course, it may require a large routine
to implement this measurement fully quantumly, since it involves inverting opera-
tors acting on the feature space. Alternatively, one can compute the desired {c,,}
classically and use the quantum computer to just measure the kernel.

6.5.3 The Impact of the Kernel on Regularisation

In statistical learning theory, the role of the regulariser in the regularised empirical
risk minimisation problem is to “punish” some functions and favour others. Above,
we specifically looked at regularisers of the form als f € F, which was shown
to be equivalent to minimising the norm of the measurement (or the length of the
vectorised measurement) in feature space. But what is it exactly that we are penalising
here? It turns out that the kernel does not only fix the space of quantum models
themselves, but also defines which functions are penalised in regularised empirical
tisk minimisation problems. This is beautifully described in [11] Sect.4.3, and we
will only give a quick overview here.

To understand regularisation, we need to have a closer look at the regularising
term WT: = (f, f). But with the construction of the RKHS it remains very opaque
what this inner product actually computes. It turns out that for every RKHS F there
is a transformation Y : F — L2(A) that maps functions in the RKHS to square
integrable functions on ¥. What we gain is a more intuitive inner product formed by
an integral

Orr rhe= I (rF(x))dx. (6.66)

The operator Y can be understood as extracting the information from the model
f which gets integrated over in the usual L2 norm, and hence penalised during
optimisation. For example, for some kernels, this can be shown to be the derivative
of functions, and regularisation, therefore, provably penalise models with “large”
higher-order derivatives—which means it favours smooth functions.

The important point is that every kernel defines a unique transformation TY, and
therefore, a unique kind of regularisation. This is summarised in Theorem 4.9in[11],
which we will reprint here without proof.


240 6 Quantum Models as Kernel Methods

Theorem 6.7 (RKHS and Regularisation Operators) For every RKHS with repro-
ducing kernel k, there exists a corresponding regularisation operator Y : F + D
(where D is an inner product space) such that for all f € F

(TKO, +), T£C))o = F(X), (6.67)

and in particular
(Tax, +), TKO’) = K(x, x’). (6.68)

Likewise, for every regularisation operator Y : F — D, where F is some function
space equipped with a dot product, there exists a corresponding RKHS F with repro-
ducing kernel K such that these two equations are satisfied.

In short, the quantum kernel or data-encoding strategy does not only tell us about
universality and optimal measurements, it also fixes the regularisation properties in
empirical risk minimisation.

6.5.4 Kernel-Based Learning Is Suprisingly Simple

Besides the representer theorem, a second main achievement of kernel theory is to
recognise that optimising the empirical risk of convex loss functions over functions
in an RKHS can be formulated as a finite-dimensional convex optimisation problem
(or in less cryptic language, optimising over extremely large spaces is surprisingly
easy when we use training data, something noted in [9] before).

The fact that the optimisation problem is finite-dimensional—and we will see the
dimension is equal to the number of training data—is important, since the feature
spaces in which the model classifies the data are usually very high-dimensional,
and possibly even infinite-dimensional. This is obviously true for the data-encoding
feature space of quantum computations as well—which is precisely why variational
quantum machine learning parametrise circuits with a small number of trainable
parameters instead of optimising over all unitaries/measurements. But even if we
optimise over all quantum models, the results of this section guarantee that the
dimensionality of the problem is limited by the size of the training data set.

The fact that optimisation is convex means that there is only one global minimum,
and that we have a lot of tools to find it [21 ]—in particular, more tools than mere gra-
dient descent. Convex optimisation problems can be roughly solved in time O(M?)
in the number of training data. Although prohibitive for large datasets, it makes
the optimisation guaranteed to be tractable (and below we will see that quantum
computers could in principle help to train with a runtime of O(M)).

Let us make the statement more precise. Again, it follows from the fact that
optimising over the RKHS of the quantum kernel is equivalent to optimising over
the space of quantum models.


6.5 Kernel-Based Training 241

Theorem 6.8 (Training quantum models can be formulated as a finite-dimensional
convex program) Let ¥ be a data domain and Y an output domain, L : X x Y x
R — [0, oc) be a loss function, F the RKHS of the quantum kernel over a non-
empty convex set X with the reproducing kernel K. Furthermore, let X > 0 be a
regularisation parameter and D = {(x", y"),m =1,...,M} C & x Ya training
data set. The regularised empirical risk minimisation problem is finite-dimensional,
and if the loss is convex, it is also convex.

Proof Recall that according to the representer Theorem 6.5, the solution to the
regularised empirical risk minimisation problem

fon = inl AILS + Ruf) (6.69)

has a representation of the form

fon (*) = D> amtr{o(x")p(x)). (6.70)
We can therefore write
x 1 '
Ripa y SOLO, ys Do mex, x). (6.71)

If the loss L is convex, then this term is also convex, and it is M-dimensional since
it only involves the M degrees of freedom a.
Now let us turn to the regularisation term and try to show the same. Consider

WSF = Do mente p(x") o(x™)} = YP aman (x, x") = aT Ka, (6.72)

mm mm’

where K € R”*™ is the kernel matrix or Gram matrix with entries Kmm =
K(x, x"), and a = (ay,..., @m) is the vector of coefficients a. Since K is by
definition of the kernel positive definite, this term is also convex. Both a and K are
furthermore finite-dimensional.

Together, training a quantum model to find the optimal solution from Eq. (6.51)
can be done by solving the optimisation problem

1 F :

inf, yp LOM Dann, x™)) + Ne Kew, (6.73)
m m

which optimises over M trainable parameters, and is convex for convex loss functions.

The support vector machine can be constructed as a special case of kernel-based
training which uses a special convex loss function, namely the hinge loss, for L


242 6 Quantum Models as Kernel Methods
L(f (x), y) = max(0, 1 — f(x)y), (6.74)

where one assumes that y € {—1, 1}. As derived in Sect. 2.5.4.3, the resulting opti-
misation problem can be constructed from geometric arguments as maximising the
“soft” margin of the closest vectors to a decision boundary. Under this loss, Eq. (6.73)
reduces to '
Cop, = max Ym — 5 Dancin y"¥" (x, 2"). (6.75)
m

myn

Training a support vector machine with hinge loss and a quantum kernel & is equiva-
lent to finding the general quantum model that minimises the hinge loss. The “quan-
tum support vector machine” in [2, 3] is, therefore, not one of many ideas to build
a hybrid classifier, it is a generic blueprint of how to train quantum models in a
kernel-based manner.

6.6 Comparing Kernel-Based and Variational Training

The fact that quantum models can be formulated as kernel methods with a quantum
kernel raises an important question for current quantum machine learning research:
how do kernel-based models, i.e., solutions to the problem in Eq. (6.73), compare to
models whose measurements are trained variationally? Let us revisit Fig. 6.4 in light
of the results of the previous section.

We saw in Sect. 6.5.4 how kernel-based training optimises the measurement over
a subspace spanned by M encoded training inputs by finding the best coefficients
Qm,m = 1...M. We also saw in Sect. 6.5.2 that this subspace contains the globally
optimal measurement. Variational training instead optimises over a subspace defined
by the parametrised ansatz, which may or may not overlap with the training data sub-
space, and could, therefore, not have access to the global optimum. The advantages
of kernel-based training are, therefore, that we are guaranteed to find the globally
optimal measurement over all possible quantum models. If the loss is convex, the
optimisation problem is furthermore of a favourable structure that comes with a lot
of guarantees about the performance and convergence of optimisation algorithms.
But besides these great properties, in classical machine learning with big data, kernel
methods were superseded by neural networks or approximate kernel methods [22]
because of their poor scaling. Training involves computing the pairwise distances
between all training data in the Gram matrix of Eq. (6.73), which has at least a run-
time of O(M?) in the number of training samples M.? In contrast, training neural
networks takes time O(M) that only depends linearly on the number of training

3 Note that this is also true when using the trained model for predictions, where we need to compute
the distance between a new input to any training input in feature space as shown in Eq. (6.51).
However, in maximum margin classifiers, or support vector machines in the stricter sense, MOSt Qm
coefficients are zero, and only the distances to a few “support vectors” are needed.


6.6 Comparing Kernel-Based and Variational Training 243

samples. Can the training of variational quantum circuits offer a similar advantage
over kernel-based training?

The answer is that it depends. So far, training variational circuits with gradient-
based methods on hardware is based on so-called parameter-shift rules (23, 24]
instead of backpropagation. This strategy introduces a linear scaling with the num-
ber of parameters |6|, and the number of circuits that need to be evaluated to train a
variational quantum model, therefore, grows with O(|9|M). If the number of param-
eters in an application grows sufficiently slowly with the dataset size, variational
circuits will almost be able to match the good scaling behaviour of neural networks,
which is an important advantage over kernel-based training. But if, like in neural
networks, the number of parameters in a variational ansatz grows linearly with the
number of data, variational quantum models end up having the same quadratic scaling
as the kernel-based approach regarding the number of circuits to evaluate. Practi-
cal experiments with 10-20 parameters and about 100 data samples show that the
constant overhead of gradient calculations on hardware make kernel-based training
in fact much faster for small-scale applications.’ In addition, there is no guarantee
that the final measurement is optimal, we have high-dimensional non-convex train-
ing landscapes, and the additional burden of choosing a good variational ansatz. In
conclusion, the kernel perspective is not only a powerful and theoretically appealing
alternative to think about quantum machine learning, but may also speedup current
quantum machine learning methods significantly.

As a beautiful example of the mutually beneficial relation of quantum computing
and kernel methods, the story does not end here. While all of the above is based on
models evaluated on a quantum computer but trained classically, convex optimisation
problems happen to be exactly the kind of thing quantum computers are good at [25].
We can, therefore, ask whether quantum models could not in principle be trained by
quantum algorithms. “In principle” alludes to the fact that such algorithms would
likely be well beyond the reach of near-term devices, since training is a more complex
affair that requires fully error-corrected quantum computers which we do not have
yet.

The reasons why quantum training could help to lower this scaling are hidden
in results from the carly days of quantum machine learning, when quantum-based
training was actively studied in the hope of finding exponential speedups for classical
machine learning [26-28]. While these speedups only hold up under very strict
assumptions of data loading oracles, they imply quadratic speedups for rather general
settings. They can be summarised as follows: given a feature map implemented by
a fault-tolerant quantum computer, we can train kernel methods in time that grows
linearly in the data. If a kernel can be implemented as a quantum computation (like
the Gaussian kernel [17]), this speedup would also hold for “classical models”—
which are then merely run on a quantum computer.

Of course, fault-tolerant quantum computers may still take many years to develop
and are likely to have a large constant overhead due to the expensive nature of
quantum error correction. But in the longer term, this shows that the use of quantum

4 Sce https://pennylane.ai/qml/demos/tutorial_kernel_based_training.html.


244 6 Quantum Models as Kernel Methods

computing is not only to implement interesting kernels. Quantum computers have the
potential to become a game changer for kernel-based machine learning in a similar
way to how GPU-accelerated hardware enabled deep learning.

References

1. Schuld, M.: Quantum machine learning models are kernel methods (2021). arXiv preprint
arXiv:2101.11020
2. Schuld, M., Killoran, N.: Quantum machine learning in feature Hilbert spaces (2018). arXiv
preprint arXiv:1803.07128v1
3. Havlitek, V., Cércoles, A.D., Temme, K., Harrow, A.W., Kandala, A., Chow, J.M., Gambetta,
J.M.: Supervised learning with quantum-enhanced feature spaces. Nature, 567(7747), 209-212
(2019)
4. Blank, C., Park, D.K., Rhee, J.K.K., Petruccione, F.: Quantum classifier with tailored quantum
kernel. npj Quantum Inf. 6(1), 1-7 (2020)
5. Park, D.K., Blank, C., Petruccione, F.; The theory of the quantum kernel-based binary classifier.
Phys. Lett. A, 384(21), 126422 (2020)
6. Killer, J.M., Muandet, K., Schélkopf, B.: Quantum mean embedding of probability distribu-
tions. Phys. Rev. Res. 1(3), 033159 (2019)
7. Lloyd, S., Schuld, M., Ijaz, A., Izaac, J., Killoran, N.: Quantum embeddings for machine
learning (2020). arXiv preprint arXiv:2001.03622
8. Liu, Y., Arunachalam, S., Temme, K.: A rigorous and robust quantum speed-up in supervised
machine learning (2020). arXiv preprint arXiv:2010.02174
9. Huang, H.Y., Broughton, M., Mohseni, M., Babbush, R., Boixo, S., Neven, H., McClean, J.R.:
Power of data in quantum machine learning (2020). arXiv preprint arXiv:201 1.01938
10. Steinwart, I., Christman, A.: Support Vector Machines. Springer Science & Business Media
(2008)
11. Schélkopf, B., Smola, A.J.: Learning With Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press (2002)
12. Pérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E., Latorre, J.I.: Data re-uploading for a
universal quantum classifier. Quantum 4, 226 (2020)
13. McClean, J.R., Boixo, S., Smelyanskiy, V.N., Babbush, R., Neven, H.: Barren plateaus in
quantum neural network training landscapes. Nat. Commun. 9(1), 1-6 (2018)
14. Wolf, M.: Quantum channels and operations: Guided tour (2012). https://www-m5.ma.tum.
de/foswiki/pub/M5/Allgemeines/MichaelWolf/QChannelLecture.pdf
15. Jagadish, V., Pewruccione, F.: An invitation to quantum channels. Quanta 7(1), 54-67 (2018)
16. Bergholm, V., Izaac, J., Schuld, M., Gogolin, C., Blank, C., McKiernan, K., Killoran, N.:
Pennylane: Automatic differentiation of hybrid quantum-classical computations (2018). arXiv
preprint arXiv:1811.04968
17. Chatterjee, R., Yu, T.: Generalized coherent states, reproducing kernels, and quantum support
vector machines (2016). arXiv preprint arXiv:1612.03713
18. Schuld, M., Sweke, R., Meyer, J.J.: The effect of data encoding on the expressive power of
variational quantum machine learning models (2020). arXiv preprint arXiv:2008.08605
19. Benedetti, M., Garcia-Pintos, D., Perdomo, O., Leyton-Ortega, V., Nam, Y., Perdomo-Ortiz,
A.: A generative modeling approach for benchmarking and training shallow quantum circuits.
npj Quantum Inf. 5(1), 1-9 (2019)
20. Cheng, S., Chen, J., Wang, L.: Information perspective to probabilistic modeling: Boltzmann
machines versus born machines. Entropy 20(8), 583 (2018)
21. Boyd, S., Vandenberghe, L.: Convex Optimization. Cambridge University Press (2004)
22. Rahimi, A., Recht, B., ctal.: Random features for large-scale kernel machines. In: NIPS, vol. 3,
p. 5. Citeseer (2007)


References 245

23.

24.

25.

Mitarai, K., Negoro, M., Kitagawa, M., Fujii, K.: Quantum circuit learning (2018). arXiv
preprint arXiv:1803.00745

Schuld, M., Bergholm, V., Gogolin, C., Izaac, J., Killoran, N.: Evaluating analytic gradients
on quantum hardware. Phys. Rev. A 99(3), (2019)

Harrow, A.W., Hassidim, A., Lloyd, S.:; Quantum algorithm for linear systems of equations.
Phys. Rev. Lett. 103(15), 150502 (2009)

. Wiebe, N., Braun, D., Lloyd, S.: Quantum algorithm for data fitting. Phys. Rev. Lett. 109(5)

(2012)

27. Rebentrost, P., Mohseni, M., Lloyd, S.: Quantum support vector machine for big data classifi-

cation. Phys. Rev. Lett. 113 (2014)
Lloyd, S., Mohseni, M., Rebentrost, P.: Quantum principal component analysis. Nat. Phys. 10,
631-633 (2014)
